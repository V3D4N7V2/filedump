{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune Llama-3 with LLaMA Factory\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ],
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "lr7rB3szzhtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giM74oK1rRIH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU environment"
      ],
      "metadata": {
        "id": "H9RXn_YQnn9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ],
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Identity Dataset"
      ],
      "metadata": {
        "id": "TeYs5Lz-QJYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"LLM\"\n",
        "AUTHOR = \"V\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "ap_fvMBsQHJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via LLaMA Board"
      ],
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/LLaMA-Factory/\n",
        "# !GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "YLsdS6V5yUMy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ],
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# args = dict(\n",
        "#   stage=\"sft\",                        # do supervised fine-tuning\n",
        "#   do_train=True,\n",
        "#   # model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "#   # https://huggingface.co/unsloth/Qwen2.5-3B-Instruct-bnb-4bit\n",
        "#   model_name_or_path=\"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\",\n",
        "#   dataset=\"output2XOXO\",             # use alpaca and identity datasets\n",
        "#   template=\"qwen\",                     # use llama3 prompt template\n",
        "#   finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
        "#   lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
        "#   output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
        "#   per_device_train_batch_size=2,               # the batch size\n",
        "#   gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
        "#   lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
        "#   logging_steps=10,                      # log every 10 steps\n",
        "#   warmup_ratio=0.1,                      # use warmup scheduler\n",
        "#   save_steps=1000,                      # save checkpoint every 1000 steps\n",
        "#   learning_rate=5e-5,                     # the learning rate\n",
        "#   num_train_epochs=3.0,                    # the epochs of training\n",
        "#   max_samples=500,                      # use 500 examples in each dataset\n",
        "#   max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
        "#   loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
        "#   fp16=True,                         # use float16 mixed precision training\n",
        "#   use_liger_kernel=True,                   # use liger kernel for efficient training\n",
        "# )\n",
        "\n",
        "# json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "# %cd /content/LLaMA-Factory/\n",
        "\n",
        "# !llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "id": "CS0Qk5OR0i4Q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # archive /content/LLaMA-Factory/llama3_lora\n",
        "\n",
        "# !zip -r llama3_lora.zip /content/LLaMA-Factory/llama3_lora\n",
        "# # download zip\n"
      ],
      "metadata": {
        "id": "JTG3lgWW7gPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer the fine-tuned model"
      ],
      "metadata": {
        "id": "PVNaC-xS5N40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install numba\n",
        "\n",
        "# from numba import cuda\n",
        "# device = cuda.get_current_device()\n",
        "# device.reset()"
      ],
      "metadata": {
        "id": "1Z3blW-0tk8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/file/d/1F1sy6uHNjP_twAPUzSfFOuXGVVodnNXb/view?usp=sharing"
      ],
      "metadata": {
        "id": "eEAqGIPB2CiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/V3D4N7V2/filedump/refs/heads/main/database_mapping.csv\n",
        "import pandas as pd\n",
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "# Navigate to the LLaMA-Factory directory\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "# Define model arguments\n",
        "args = dict(\n",
        "    model_name_or_path=\"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\",\n",
        "    template=\"qwen\",\n",
        "    adapter_name_or_path=\"llama3_lora\",\n",
        "    finetuning_type=\"lora\",\n",
        "    quantization_bit=4,\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "# Read the CSV file\n",
        "input_file = \"database_mapping.csv\"\n",
        "output_file = \"output.csv\"\n",
        "data = pd.read_csv(input_file)\n",
        "\n",
        "# Prepare a list to store results\n",
        "results = []\n",
        "\n",
        "# Iterate through each row in the CSV\n",
        "for _, row in data.iterrows():\n",
        "    schema = row['database_structure']\n",
        "    natural_language_query = row['question']\n",
        "    instance_id = row['instance_id']\n",
        "    db_name = row['db']\n",
        "\n",
        "    # Construct the prompt\n",
        "    prompt = (\n",
        "        \"Using the following database schema, write an SQL query to perform the operation described below:\\n\"\n",
        "        f\"Schema:\\n{schema}\\n\"\n",
        "        f\"Natural Language Query: {natural_language_query}\\n\"\n",
        "    )\n",
        "\n",
        "    # Create a message structure for the model\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # Generate the response\n",
        "    print(f\"Processing instance_id: {instance_id}...\")\n",
        "    response = \"\"\n",
        "    for new_text in chat_model.stream_chat(messages):\n",
        "        response += new_text\n",
        "\n",
        "    # Append the result to the results list\n",
        "    results.append({\n",
        "        \"instance_id\": instance_id,\n",
        "        \"db\": db_name,\n",
        "        \"question\": natural_language_query,\n",
        "        \"response\": response\n",
        "    })\n",
        "\n",
        "    torch_gc()  # Clear GPU memory if necessary\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "output_df = pd.DataFrame(results)\n",
        "output_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Results saved to {output_file}.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "txjKPGvMmwW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ],
      "metadata": {
        "id": "kTESHaFvbNTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "mcNcHcA4bf4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}